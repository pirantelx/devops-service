# Настройка LLM для чата с ИИ

## Описание

Система интегрирована с локальным LLM движком Ollama для ответов на вопросы по данным системы DevOps Portal.

## Требования

- Docker и Docker Compose
- Минимум 4GB RAM (рекомендуется 8GB+)
- Свободное место на диске: ~2GB для модели

## Быстрый старт

1. **Запустите систему:**
   ```bash
   docker-compose up -d
   ```

2. **Проверьте, что Ollama запущен:**
   ```bash
   docker ps | grep ollama
   ```

3. **Загрузите модель (если не загружена автоматически):**
   ```bash
   docker exec -it ollama ollama pull llama3.2:1b
   ```
   
   Или для более мощной модели:
   ```bash
   docker exec -it ollama ollama pull llama3.2
   ```

4. **Проверьте доступные модели:**
   ```bash
   docker exec -it ollama ollama list
   ```

## Настройка модели

По умолчанию используется модель `llama3.2:1b` (легкая и быстрая).

Чтобы изменить модель, установите переменную окружения в `docker-compose.yml`:

```yaml
app-devops:
  environment:
    - OLLAMA_HOST=http://ollama:11434
    - OLLAMA_MODEL=llama3.2  # или другая модель
```

Доступные модели:
- `llama3.2:1b` - самая легкая, быстрая (рекомендуется для начала)
- `llama3.2` - средняя, хорошее качество
- `mistral` - альтернативная модель
- `qwen2.5:1.5b` - китайская модель, хорошее качество

## Использование

1. Войдите в систему
2. Перейдите в раздел "Чат с ИИ"
3. Задайте вопрос по данным системы (новости, проблемы, внедрения, инфраструктура, АС/ФП)

## Как это работает

1. **RAG (Retrieval-Augmented Generation):**
   - Система загружает все данные из системы (новости, проблемы, внедрения и т.д.)
   - При получении вопроса ищет релевантные данные
   - Формирует контекст из найденных данных
   - Отправляет контекст и вопрос в LLM
   - LLM генерирует ответ на основе контекста

2. **История чата:**
   - Все сообщения сохраняются в `data/ai_chat/{username}.json`
   - История используется для контекста в следующих вопросах

## Устранение проблем

### Ollama не запускается
- Проверьте логи: `docker logs ollama`
- Убедитесь, что порт 11434 свободен

### Модель не загружается
- Проверьте доступное место на диске
- Попробуйте загрузить более легкую модель: `llama3.2:1b`

### Медленные ответы
- Используйте более легкую модель (`llama3.2:1b`)
- Увеличьте RAM для контейнера Ollama

### Ошибки подключения
- Убедитесь, что Ollama доступен по адресу `http://ollama:11434` из контейнера app-devops
- Проверьте сеть Docker: `docker network inspect devops-service_backend`

## Производительность

- **llama3.2:1b**: ~500MB RAM, быстрые ответы (~2-5 сек)
- **llama3.2**: ~2GB RAM, качественные ответы (~5-10 сек)
- **mistral**: ~2GB RAM, хорошее качество

Рекомендуется начать с `llama3.2:1b` для тестирования.

